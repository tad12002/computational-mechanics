---
jupytext:
  formats: notebooks//ipynb,md:myst
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.11.4
kernelspec:
  display_name: Python 3 (ipykernel)
  language: python
  name: python3
---

> __Content modified under Creative Commons Attribution license CC-BY
> 4.0, code under BSD 3-Clause License ¬© 2020 R.C. Cooper__

+++

# Homework

```{code-cell} ipython3
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
```

## Problems [Part 1](./01_Cheers_Stats_Beers.md)

1. Gordon Moore created an empirical prediction that the rate of
semiconductors on a computer chip would double every two years. This
prediction was known as Moore's law. Gordon Moore had originally only
expected this empirical relation to hold from 1965 - 1975
[[1](https://en.wikipedia.org/wiki/Moore%27s_law),[2](https://spectrum.ieee.org/computing/hardware/gordon-moore-the-man-whose-name-means-progress)],
but semiconductor manufacturers were able to keep up with Moore's law
until 2015. 

In the folder "../data" is a comma separated value (CSV) file,
"transistor_data.csv" [taken from wikipedia
01/2020](https://en.wikipedia.org/wiki/Transistor_count#Microprocessors).

a. Use the `!head ../data/transistor_data.csv` command to look at
the top of the csv. What are the headings for the columns?

b. Load the csv into a pandas dataframe. How many missing values
(`NaN`) are
in the column with the number of transistors? What fraction are
missing?

+++

## Part 1, Question 1.a

```{code-cell} ipython3
!head ../data/transistor_data.csv
```

The headings are Processor, MOS transistor count, Date of Introduction, Designer, MOSprocess, and Area

+++

## Part 1, Question 1.b

```{code-cell} ipython3
semiconductors = pd.read_csv('../data/transistor_data.csv')
```

```{code-cell} ipython3
type(semiconductors)
```

```{code-cell} ipython3
semiconductors
```

```{code-cell} ipython3
count_nan = semiconductors['MOS transistor count'].isna().sum()
    
print ('Count of NaN: ' + str(count_nan))
```

Fraction of NaN values is 7/186.

+++

## Problems [Part 2](./02_Seeing_Stats.md)

1. Many beers do not report the IBU of the beer because it is very
small. You may be accidentally removing whole categories of beer from
our dataset by removing rows that do not include the IBU measure. 

    a. Use the command `beers_filled = beers.fillna(0)` to clean the `beers` dataframe
    
    b. Recreate the plot "Beer ABV vs. IBU mean values by style" 
    [bubble
    plot](https://cooperrc.github.io/computational-mechanics/module_02/02_Seeing_Stats.html#bubble-chart) with `beers_filled`. What differences do you notice between the plots?

+++

# Part 2, Question 1.a

```{code-cell} ipython3
beers = pd.read_csv("../data/beers.csv")
```

```{code-cell} ipython3
beers
```

```{code-cell} ipython3
beers_filled = beers.fillna(0)
```

```{code-cell} ipython3
beers_filled
```

# Part 2, Question 1.b

```{code-cell} ipython3
ibu = beers_filled['ibu'].values
len(ibu)
```

```{code-cell} ipython3
abv = beers_filled['abv'].values
len(abv)
```

```{code-cell} ipython3
beers_styles = beers_filled.drop(['Unnamed: 0','name','brewery_id','ounces','id'], axis=1)
```

```{code-cell} ipython3
style_counts = beers_styles['style'].value_counts()
```

```{code-cell} ipython3
type(style_counts)
```

```{code-cell} ipython3
len(style_counts)
```

```{code-cell} ipython3
style_means = beers_styles.groupby('style').mean()
```

```{code-cell} ipython3
style_means
```

```{code-cell} ipython3
style_counts = style_counts.sort_index()
```

```{code-cell} ipython3
style_counts[0:10]
```

```{code-cell} ipython3
from matplotlib import cm
colors = cm.viridis(style_counts.values)
```

```{code-cell} ipython3
ax = style_means.plot.scatter(figsize=(10,10), 
                               x='abv', y='ibu', s=style_counts*20, color=colors,
                               title='Beer ABV vs. IBU mean values by style\n',
                               alpha=0.3);

for i, txt in enumerate(list(style_counts.index.values)):
    if style_counts.values[i] > 65:
        ax.annotate(txt, (style_means.abv.iloc[i],style_means.ibu.iloc[i]), fontsize=12)
```

What differences do you notice between the plots?: I notice that the bubble plot sizes are similar and represent the same numbers but the main difference is the range of x and y in that the beers_filled plot ranges its abv values starting at zero where the example starts at 0.04 abv, and the ibu values for beers_filled is 0 to 70 where as ibu in the example ranges from 20 to 100.

+++

2. Gordon Moore created an empirical prediction that the rate of
semiconductors on a computer chip would double every two years. This
prediction was known as Moore's law. Gordon Moore had originally only
expected this empirical relation to hold from 1965 - 1975
[[1](https://en.wikipedia.org/wiki/Moore%27s_law),[2](https://spectrum.ieee.org/computing/hardware/gordon-moore-the-man-whose-name-means-progress)],
but semiconductor manufacturers were able to keep up with Moore's law
until 2015. 

    In the folder "../data" is a comma separated value (CSV) file, "transistor_data.csv" [taken from wikipedia 01/2020](https://en.wikipedia.org/wiki/Transistor_count#Microprocessors). 
    Load the csv into a pandas dataframe, it has the following headings:

    |Processor| MOS transistor count| Date of Introduction|Designer|MOSprocess|Area|
    |---|---|---|---|---|---|

    a. In the years 2017, what was the average MOS transistor count? 
    Make a boxplot of the transistor count in 2017 and find the first, second and third quartiles.

    b. Create a semilog y-axis scatter plot (i.e. `plt.semilogy`) for the 
    "Date of Introduction" vs "MOS transistor count". 
    Color the data according to the "Designer".

+++

# Part 2 Question 2.a

```{code-cell} ipython3
semiconductors = pd.read_csv('../data/transistor_data.csv')

semiconductors
```

```{code-cell} ipython3
#Pull out only year 2017

semiconductors_2017 = semiconductors[semiconductors['Date of Introduction'] == 2017]
```

```{code-cell} ipython3
semiconductors_2017
```

```{code-cell} ipython3
#cleaning values MOS transistor count
transistors_series = semiconductors_2017['MOS transistor count']
transistors_clean = transistors_series.dropna()
MOS_transistor_count_clean = transistors_clean.values

# Calculate the average of the MOS transistor count values
average_count = MOS_transistor_count_clean.mean()


print("Average MOS transistor count in 2017:", average_count)
```

# Part 2 Question 2.b

```{code-cell} ipython3
# Extract the required columns
date_of_intro = semiconductors['Date of Introduction']
transistor_count = semiconductors['MOS transistor count']
designer = semiconductors['Designer']

# Get unique designers and assign a color to each
unique_designers = designer.unique()
num_designers = len(unique_designers)
colors = cm.viridis(np.linspace(0, 1, num_designers))

# Create a scatter plot with semilog y-axis and color-coded data


plt.figure(figsize=(10, 10))
for i, designer_name in enumerate(unique_designers):
    designer_mask = designer == designer_name
    plt.semilogy(date_of_intro[designer_mask], transistor_count[designer_mask], 'o', c=colors[i], label=designer_name)

plt.title('Date of Introduction vs MOS Transistor Count')
plt.xlabel('Date of Introduction')
plt.ylabel('MOS Transistor Count')


for i, txt in enumerate(list(unique_designers)):
    plt.annotate(txt, (date_of_intro[designer == txt].iloc[0], transistor_count[designer == txt].iloc[0]), fontsize=12)

# Show the plot
plt.show()
```

## Problems [Part 3](03_Linear_Regression_with_Real_Data.md)

1. There is a csv file in '../data/primary-energy-consumption-by-region.csv' that has the energy consumption of different regions of the world from 1965 until 2018 [Our world in Data](https://ourworldindata.org/energy). 
Compare the energy consumption of the United States to all of Europe. Load the data into a pandas dataframe. *Note: you can get certain rows of the data frame by specifying what you're looking for e.g. 
`EUR = dataframe[dataframe['Entity']=='Europe']` will give us all the rows from Europe's energy consumption.*

    a. Plot the total energy consumption of the United States and Europe
    
    b. Use a linear least-squares regression to find a function for the energy consumption as a function of year
    
    energy consumed = $f(t) = At+B$
    
    c. At what year would you change split the data and use two lines like you did in the 
    land temperature anomoly? Split the data and perform two linear fits. 
    
    d. What is your prediction for US energy use in 2025? How about European energy use in 2025?

+++

# Part 3, Question 1.a

```{code-cell} ipython3
#a. Plot the total energy consumption of the United States and Europe

energy = pd.read_csv('../data/primary-energy-consumption-by-region.csv')

EUR = energy[energy['Entity'] == 'Europe']
US = energy[energy['Entity'] == 'United States']

plt.plot(EUR['Year'], EUR['Primary Energy Consumption (terawatt-hours)'], label='Europe')
plt.plot(US['Year'], US['Primary Energy Consumption (terawatt-hours)'], label='United States')

plt.xlabel('Year')
plt.ylabel('Energy Consumption')
plt.title('Energy Consumption Comparison: Europe vs United States')
plt.legend()

plt.show()
```

# Part 3, Question 1.b

+++

## US Linear Reg

```{code-cell} ipython3
#b. Use a linear least-squares regression to find a function for the energy consumption as a function of year
#energy consumed =  ùëì(ùë°)=ùê¥ùë°+ùêµ

#Pull out energy consumption and its year
x_us = US['Year']
y_us = US['Primary Energy Consumption (terawatt-hours)']

xi = np.array(x_us)
yi = np.array(y_us)

#Find the mean of the year and energy consumption
x_mean = np.mean(xi)
y_mean = np.mean(yi)

#After finding mean values, compute the coefficients a1 and a0
a_1 = np.sum(yi*(xi - x_mean)) / np.sum(xi*(xi - x_mean)) 
a_0 = y_mean - a_1*x_mean


def coefficients(x, y, x_mean, y_mean):
    """
    Write docstrings here
    Arguments
    ---------
    x: independent variable 
    y: dependent variable measurements
    x_mean: mean of independent variable
    y_mean: mean of dependent variable
    Returns
    -------
    a_1:¬†the least-squares regression slope
    a_0: the least-squares regression intercept
    """
    a_1 = np.sum(yi*(xi - x_mean)) / np.sum(xi*(xi - x_mean))
    a_0 = y_mean - a_1*x_mean
    
    return a_1, a_0

reg = a_0 + a_1 * xi

plt.figure(figsize=(14, 8))

plt.plot(xi, yi,'s', color='#2929a3', linewidth=1, alpha=0.5,label='Energy Consumption') 
plt.plot(xi, reg, 'k--', linewidth=2, label='Linear regression')
plt.title('United States energy consumption')
plt.xlabel('Year')
plt.ylabel('Primary Energy Consumption (terawatt-hours)')
plt.legend(loc='best', fontsize=15)
plt.grid();
```

## Europe Linear Reg

```{code-cell} ipython3
#b. Use a linear least-squares regression to find a function for the energy consumption as a function of year
#energy consumed =  ùëì(ùë°)=ùê¥ùë°+ùêµ

#Pull out energy consumption and its year
x_eur = EUR['Year']
y_eur = EUR['Primary Energy Consumption (terawatt-hours)']

xi = np.array(x_eur)
yi = np.array(y_eur)

#Find the mean of the year and energy consumption
x_mean = np.mean(xi)
y_mean = np.mean(yi)

#After finding mean values, compute the coefficients a1 and a0
a_1 = np.sum(yi*(xi - x_mean)) / np.sum(xi*(xi - x_mean)) 
a_0 = y_mean - a_1*x_mean


def coefficients(x, y, x_mean, y_mean):
    """
    Write docstrings here
    Arguments
    ---------
    x: independent variable 
    y: dependent variable measurements
    x_mean: mean of independent variable
    y_mean: mean of dependent variable
    Returns
    -------
    a_1:¬†the least-squares regression slope
    a_0: the least-squares regression intercept
    """
    a_1 = np.sum(yi*(xi - x_mean)) / np.sum(xi*(xi - x_mean))
    a_0 = y_mean - a_1*x_mean
    
    return a_1, a_0

reg = a_0 + a_1 * xi

plt.figure(figsize=(14, 8))

plt.plot(xi, yi,'s', color='#2929a3', linewidth=1, alpha=0.5,label='Energy Consumption') 
plt.plot(xi, reg, 'k--', linewidth=2, label='Linear regression')
plt.title('Europe energy consumption')
plt.xlabel('Year')
plt.ylabel('Primary Energy Consumption (terawatt-hours)')
plt.legend(loc='best', fontsize=15)
plt.grid();
```

# Part 3, Question 1.c

+++

## US Split regression

```{code-cell} ipython3
#At what year would you change split the data and use two lines like you did in the land temperature anomoly? Split the data and perform two linear fits.

#Split the Europe data set at 1985 and the US data set at 1997

np.where(xi==1997)
```

```{code-cell} ipython3
year = x_us
Primary_Energy_Consumption = y_us

year_1 , Primary_Energy_Consumption_1 = year[0:32], Primary_Energy_Consumption[0:32]
year_2 , Primary_Energy_Consumption_2 = year[32:], Primary_Energy_Consumption[32:]

m1, b1 = np.polyfit(year_1, Primary_Energy_Consumption_1, 1)
m2, b2 = np.polyfit(year_2, Primary_Energy_Consumption_2, 1)

f_linear_1 = np.poly1d((m1, b1))
f_linear_2 = np.poly1d((m2, b2))

plt.figure(figsize=(10, 5))

plt.plot(year, Primary_Energy_Consumption, color='#2929a3', linestyle='-', linewidth=1, alpha=0.5) 
plt.plot(year_1, f_linear_1(year_1), 'g--', linewidth=2, label='1880-1969')
plt.plot(year_2, f_linear_2(year_2), 'r--', linewidth=2, label='1970-2016')

plt.xlabel('Year')
plt.ylabel('Primary Energy Consumption (terawatt-hours)',  fontsize = 13)
plt.legend(loc='best', fontsize=15)
plt.grid();
```

## EUR Split regression

```{code-cell} ipython3
np.where(xi==1985)
```

```{code-cell} ipython3
year = x_eur
Primary_Energy_Consumption = y_eur

year_1 , Primary_Energy_Consumption_1 = year[0:20], Primary_Energy_Consumption[0:20]
year_2 , Primary_Energy_Consumption_2 = year[20:], Primary_Energy_Consumption[20:]

m1, b1 = np.polyfit(year_1, Primary_Energy_Consumption_1, 1)
m2, b2 = np.polyfit(year_2, Primary_Energy_Consumption_2, 1)

f_linear_1 = np.poly1d((m1, b1))
f_linear_2 = np.poly1d((m2, b2))

plt.figure(figsize=(10, 5))

plt.plot(year, Primary_Energy_Consumption, color='#2929a3', linestyle='-', linewidth=1, alpha=0.5) 
plt.plot(year_1, f_linear_1(year_1), 'g--', linewidth=2, label='1880-1969')
plt.plot(year_2, f_linear_2(year_2), 'r--', linewidth=2, label='1970-2016')

plt.xlabel('Year')
plt.ylabel('Primary Energy Consumption (terawatt-hours)', fontsize = 13)
plt.legend(loc='best', fontsize=14)
plt.grid();
```

# Part 3, Question 1.d

+++

## What is your prediction for US energy use in 2025? How about European energy use in 2025?

### For the US, 26000 terra-watt hours

### For Europe, 24000 terra-watt hours

+++

2. You plotted Gordon Moore's empirical prediction that the rate of semiconductors on a computer chip would double every two years in [02_Seeing_Stats](./02_Seeing_Stats). This prediction was known as Moore's law. Gordon Moore had originally only expected this empirical relation to hold from 1965 - 1975 [[1](https://en.wikipedia.org/wiki/Moore%27s_law),[2](https://spectrum.ieee.org/computing/hardware/gordon-moore-the-man-whose-name-means-progress)], but semiconductor manufacuturers were able to keep up with Moore's law until 2015. 

Use a linear regression to find our own historical Moore's Law.    

Use your code from [02_Seeing_Stats](./02_Seeing_Stats) to plot the semilog y-axis scatter plot 
(i.e. `plt.semilogy`) for the "Date of Introduction" vs "MOS transistor count". 
Color the data according to the "Designer".

Create a linear regression for the data in the form of 

$log(transistor~count)= f(date) = A\cdot date+B$

rearranging

$transistor~count= e^{f(date)} = e^B e^{A\cdot date}$

You can perform a least-squares linear regression using the following assignments

$x_i=$ `dataframe['Date of Introduction'].values`

and

$y_i=$ as `np.log(dataframe['MOS transistor count'].values)`

a. Plot your function on the semilog y-axis scatter plot

b. What are the values of constants $A$ and $B$ for our Moore's law fit? How does this compare to Gordon Moore's prediction that MOS transistor count doubles every two years?

```{code-cell} ipython3

```

```{code-cell} ipython3
data = pd.read_csv('../data/transistor_data.csv')
data = data.dropna()
xi=data['Date of Introduction'].values
TC=data['MOS transistor count'].values
```

## Problems [Part 4](04_Stats_and_Montecarlo.md)

__1.__ [Buffon's needle problem](https://en.wikipedia.org/wiki/Buffon) is
another way to estimate the value of $\pi$ with random numbers. The goal
in this Monte Carlo estimate of $\pi$ is to create a ratio that is close
to [3.1415926...](http://www.math.com/tables/constants/pi.htm) _similar
to the example with darts points lying inside/outside a unit circle
inside a unit square._ 

![Buffon's needle for parallel
lines](https://upload.wikimedia.org/wikipedia/commons/f/f6/Buffon_needle.gif)

In this Monte Carlo estimation, you only need to know two values:
- the distance from line 0, $x = [0,~1]$
- the orientation of the needle, $\theta = [0,~2\pi]$

The y-location does not affect the outcome of crosses line 0 or not
crossing line 0. 

__a.__ Generate 100 random `x` and `theta` values _remember_ $\theta =
[0,~2\pi]$

__b.__ Calculate the x locations of the 100 needle ends e.g. $x_end = x
\pm \cos\theta$ _since length is unit 1. 

__c.__ Use 
[`np.logical_and`](https://numpy.org/doc/stable/reference/generated/numpy.logical_and.html)
to find the number of needles that have minimum $x_{end~min}<0$ and
maximum $x_{end~max}>0$. The ratio
$\frac{x_{end~min}<0~and~x_{end~max}>0}{number~of~needles} =
\frac{2}{\pi}$ _for large values of $number~of~needles$_.

__2.__ Build a random walk data set with steps between $dx = dy =
-1/2~to~1/2~m$. If 100 particles take 10 steps, calculate the number of
particles that move further than 0.5 m. 

_Bonus: Can you do the work without any `for`-loops? Change the size of
`dx` and `dy` to account for multiple particles._

+++

# Part 4, Question 1.a

```{code-cell} ipython3
M = 100   # 100 point length

# Generating 100 random values of x and theta
x, theta = [], []
for i in range(M):
    x.append(random.random())
    theta.append(random.random() * math.pi * 2)
```

# Part 4, Question 1.b and 1.c

```{code-cell} ipython3
# Calculating locations of M needles
x_count = 0
for i in range(M):
    x_left =min( x[i] - math.cos(theta[i]), x[i] + math.cos(theta[i]))
    x_right =max( x[i] - math.cos(theta[i]), x[i] + math.cos(theta[i]))
    #print(x_left, x_right)
    if np.logical_and( x_left < 0 , x_right > 0 ):
        x_count += 1

# 2/pi value
result = x_count/M

# pi value
pi_obtained = 2/result

print("Pi obtained: ", pi_obtained)
```

# Part 4, Question 2

```{code-cell} ipython3
num_particles = 100
num_steps = 10
step_range = (-0.5, 0.5)
distance_threshold = 0.5

count_particles_beyond_threshold = 0
final_positions = []

for _ in range(num_particles):
    position = [0, 0]
    for _ in range(num_steps):
        dx = random.uniform(step_range[0], step_range[1])
        dy = random.uniform(step_range[0], step_range[1])
        position[0] += dx
        position[1] += dy
    
    distance = math.sqrt(position[0]**2 + position[1]**2)
    if distance > distance_threshold:
        count_particles_beyond_threshold += 1
    
    final_positions.append(position)

print("Number of particles that move further than 0.5 m:", count_particles_beyond_threshold)
```

__3.__ 100 steel rods are going to be used to support a 1000 kg structure. The
rods will buckle when the load in any rod exceeds the [critical buckling
load](https://en.wikipedia.org/wiki/Euler%27s_critical_load)

$P_{cr}=\frac{\pi^3 Er^4}{16L^2}$

where E=200e9 Pa, r=0.01 m +/-0.001 m, and L is the 
length of the rods supporting the structure. Create a Monte
Carlo model `montecarlo_buckle` that predicts 
the mean and standard deviation of the buckling load for 100
samples with normally distributed dimensions r and L. 

```python
mean_buckle_load,std_buckle_load=\
montecarlo_buckle(E,r_mean,r_std,L,N=100)
```

__a.__ What is the mean_buckle_load and std_buckle_load for L=5 m?

__b.__ What length, L, should the beams be so that only 2.5% will 
reach the critical buckling load?

+++

# Part 4, Question 3.a

```{code-cell} ipython3
def montecarlo_buckle(E, r_mean, r_std, L, N=100):
    '''Generate N rods of length L with radii of r=r_mean+/-r_std
    then calculate the mean and std of the buckling loads in for the
    rod population holding a 1000-kg structure
    Arguments
    ---------
    E: Young's modulus [note: keep units consistent]
    r_mean: mean radius of the N rods holding the structure
    r_std: standard deviation of the N rods holding the structure
    L: length of the rods (or the height of the structure)
    N: number of rods holding the structure, default is N=100 rods
    Returns
    -------
    mean_buckle_load: mean buckling load of N rods under 1000*9.81/N-Newton load
    std_buckle_load: std dev buckling load of N rods under 1000*9.81/N-Newton load
    '''
    pc = np.pi**2 * E / (4 * L**2)
    r = np.random.normal(r_mean, r_std, N)
    P = np.pi**2 * E * r**4 / (4 * L**2)
    
    mean_buckle_load = np.mean(P)
    std_buckle_load = np.std(P)
    
    return np.mean(P), np.std(P)

E = 200e9
r_mean = 0.01
r_std = 0.001
N = 100

#What is the mean_buckle_load and std_buckle_load for L=5 m?
L = 5
mean_buckle_load, std_buckle_load = montecarlo_buckle(E, r_mean, r_std, L, N)
print("Mean buckling load for L = 5m:", mean_buckle_load)
print("Standard deviation of buckling load for L = 5m:", std_buckle_load)
```

# Part 4, Question 3.b

```{code-cell} ipython3
#What length, L, should the beams be so that only 2.5% will reach the critical buckling load?
target_prob = 0.025
L = 0.5 * np.pi * r_mean / np.sqrt(target_prob * r_std**2 / (np.pi**2 * E))
print("Length of rods to have 2.5% reaching critical buckling load:", L)
```
